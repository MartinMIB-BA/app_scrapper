import json
import os
import datetime
import instaloader
import base64
import time
import random

# Configuration: Map Trail Name to Instagram Username
# NOTE: Using public web access or strict login is tricky. 
# For GitHub Actions, we might use a session file or just public access if possible (often blocked).
# Alternative: Use "rss.app" feeds if the user has them (some were in trails.dart), 
# or use a lighter scraping approach for just the latest caption.

# MAPPING extracted from trails.dart:
TRAIL_config = {
    "Traily Biely Kríž": "trailbielykriz", # guesswork, need to verify
    "Baťove traily": "batovetraily",
    "Bikepark Kálnica": "bikeparkkalnica",
    "Trail park Dolní Morava": "dolnimorava", # ?
    "Nitrails": "nitrails",
    "Bojnické traily": "hornonitrianske_stopy_bojnice", # check web
    "Bikepark Jasenská": "bikeparkjasenska",
    "Žilina Oko Trails": "okotrails",
    "Laskomerské singletraily": "laskomerske_singletraily", # check facebook/ig
    "Malinô Brdo Bikepark": "bikepark_malino_brdo", # check
    "Kubínska Hoľa Bikepark": "bikeparkkubinska",
    "Bachledka Bike Park": "bachledka_ski_sun",
    "Bikepark Mýto pod Ďumbierom": "kopeczabavy",
    "Bikepark Drozdovo": "skidrozdovo_bikepark",
    "Babské Traily": "babske_traily",
    "DIVO Traily": "divo_traily",
    "Bikepark Koliba": "bikeparkkoliba",
    "KeCy Košické Cyklotraily": "kecy_kosicke_cyklotraily",
}

# Placeholder for the actual scraping logic
# We will ideally use `instaloader` in the GitHub Action.

def mock_scrape():
    """
    Creates a dummy JSON structure for testing the UI.
    In the real GitHub Action, this will be replaced by actual scraping.
    """
    results = {}
    json_path = "assets/data/instagram_feed.json"
    
    # 1. LOAD EXISTING DATA (To preserve it in case of failure)
    if os.path.exists(json_path):
        try:
            with open(json_path, "r", encoding="utf-8") as f:
                results = json.load(f)
            print("Loaded existing data. Will preserve entries if scraping fails.")
        except Exception as e:
            print(f"Error loading existing JSON: {e}")

    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")
    
    display_names = {
        "Bikepark Kálnica": "bikeparkkalnica",
        "Traily Biely Kríž": "_trailbk",
        "Baťove traily": "batovetraily",
        "Bikepark Drozdovo": "bikeparkdrozdovo",
        "Bojnické traily": "hornonitrianskestopy",
        "DIVO Traily": "divotrailytrencin",
        "Bikepark Jasenská": "bikepark_jasenska",
        "Žilina Oko Trails": "zilina_okotrails",
        "Malinô Brdo Bikepark": "bikepark_malino_brdo",
        "Trail park Žehra": "trailparkzehra",
        "Babské Traily": "babske_traily",
        "KeCy Košické Cyklotraily": "kecy_kosicke_cyklotraily"
    }

    # Initialize Instaloader
    try:
        L = instaloader.Instaloader()
        
        # Load session from env var if available (GitHub Action)
        session_data = os.environ.get("IG_SESSION_DATA")
        session_user = os.environ.get("IG_SESSION_USER")
        
        if session_data and session_user:
            # Decode session file content
            try:
                # We expect base64 encoded bytes of the session file
                session_bytes = base64.b64decode(session_data)
                
                # Instaloader expects a file named 'session-{username}'
                session_filename = f"session-{session_user}"
                with open(session_filename, "wb") as f:
                    f.write(session_bytes)
                
                print(f"Decoding session for {session_user}...")
                L.load_session_from_file(session_user, filename=session_filename)
                print("Session loaded successfully!")
            except Exception as e:
                print(f"Error loading session from env: {e}")
        
    except Exception as e:
        print(f"Instaloader error init: {e}")
        L = None

    for trail_name, handle in TRAIL_config.items():
        # Human-like delay between profiles (15 to 45 seconds)
        if L: 
            sleep_time = random.randint(15, 45)
            print(f"Waiting {sleep_time}s to act like a human... ☕")
            time.sleep(sleep_time)

        # Only scrape the ones we have specific handles for in display_names (active set)
        if trail_name in display_names and L:
            target_handle = display_names[trail_name]
            print(f"Scraping real data for {trail_name} (@{target_handle})...")
            try:
                profile = instaloader.Profile.from_username(L.context, target_handle)
                posts = profile.get_posts()
                
                # Fetch top 10 posts to have enough candidates after filtering pinned/old
                candidates = []
                count = 0
                for p in posts:
                    candidates.append(p)
                    count += 1
                    if count >= 10: break
                
                if not candidates:
                    raise Exception("No posts found")

                # Sort by date descending (newest first)
                candidates.sort(key=lambda x: x.date_local, reverse=True)
                
                # Take top 7
                final_posts = candidates[:7]

                posts_data = []
                # Ensure directory exists
                os.makedirs("assets/data/instagram", exist_ok=True)
                
                for post in final_posts:
                    # Download Image
                    img_url = post.url
                    local_filename = f"{post.shortcode}.jpg"
                    local_path = os.path.join("assets/data/instagram", local_filename)
                    
                    try:
                        # Use requests to download
                        import requests
                        response = requests.get(img_url, timeout=10)
                        if response.status_code == 200:
                            with open(local_path, "wb") as f:
                                f.write(response.content)
                            # SAVE AS GITHUB RAW URL (Live Update)
                            # Repo: MartinMIB-BA/app_scrapper
                            final_image_url = f"https://raw.githubusercontent.com/MartinMIB-BA/app_scrapper/main/assets/data/instagram/{local_filename}"
                        else:
                            print(f"    Failed to download image: {response.status_code}")
                            final_image_url = img_url # Fallback to remote if download fails
                    except Exception as e:
                        print(f"    Download error: {e}")
                        final_image_url = img_url

                    posts_data.append({
                        "caption": post.caption if post.caption else "Bez popisu",
                        "date": post.date_local.strftime("%Y-%m-%d %H:%M"),
                        "timestamp": int(post.date_local.timestamp()), 
                        "url": f"https://www.instagram.com/p/{post.shortcode}/",
                        "image_url": final_image_url 
                    })

                # SUCCESS: Overwrite the entry with new data
                results[trail_name] = {
                    "username": target_handle,
                    "posts": posts_data
                }
                print(f"  -> Success! Found {len(posts_data)} posts for {trail_name}. Images downloaded.")
                
            except Exception as e:
                print(f"  -> Error scraping {target_handle}: {e}")
                # FAILURE: Do NOT blank out the entry. Keep `results[trail_name]` as it was loaded from file.
                if trail_name not in results:
                    # If we don't have old data, we have to provide empty fallback
                    results[trail_name] = {
                        "username": target_handle,
                        "posts": []
                    }

    
    os.makedirs("assets/data", exist_ok=True)
    with open("assets/data/instagram_feed.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print("Data saved to assets/data/instagram_feed.json")

if __name__ == "__main__":
    mock_scrape()
